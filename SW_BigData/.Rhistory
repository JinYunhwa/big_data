n=n())
#11번
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT =sum(AMT),
MEAN_AMT =mean(AMT),
MIN_AMT =min(AMT),
MAX_AMT=max(AMT),
n=n())
install.packages("dplyr")
library(dplyr)
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT =sum(AMT),
MEAN_AMT =mean(AMT),
MIN_AMT =min(AMT),
MAX_AMT=max(AMT),
n=n())
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT =sum(AMT),
MEAN_AMT =mean(AMT),
MIN_AMT =min(AMT),
MAX_AMT=max(AMT),
n=n())
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT <- sum(AMT),
MEAN_AMT <- mean(AMT),
MIN_AMT <- min(AMT),
MAX_AMT <- max(AMT),
n=n())
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT <- sum(AMT),
MEAN_AMT <- mean(AMT),
MIN_AMT <- min(AMT),
MAX_AMT <- max(AMT),
n=n())
#11번
sample1 %>%
group_by(Gender) %>%
summarise(Sum_AMT =sum(AMT),
MEAN_AMT =mean(AMT),
MIN_AMT =min(AMT),
MAX_AMT=max(AMT),
n=n())
#11번
sample1 %>%
group_by(Gender) %>%
summarise(Sum_AMT =sum(AMT),
MEAN_AMT =mean(AMT),
MIN_AMT =min(AMT),
MAX_AMT=max(AMT),
n=n())
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT <- sum(AMT),
MEAN_AMT <- mean(AMT),
MIN_AMT <- min(AMT),
MAX_AMT <- max(AMT),
n=n())
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT <- sum(AMT),
MEAN_AMT <- mean(AMT),
MIN_AMT <- min(AMT),
MAX_AMT <- max(AMT),
n=n())
library(dplyr)
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT <- sum(AMT),
MEAN_AMT <- mean(AMT),
MIN_AMT <- min(AMT),
MAX_AMT <- max(AMT),
n=n())
library(dplyr)
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT <- sum(AMT),
MEAN_AMT <- mean(AMT),
MIN_AMT <- min(AMT),
MAX_AMT <- max(AMT),
n=n())
View(readxl_data)
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT <- sum(AMT17),
MEAN_AMT <- mean(AMT),
MIN_AMT <- min(AMT),
MAX_AMT <- max(AMT),
n=n())
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT <- sum(AMT17),
MEAN_AMT <- mean(AMT17),
MIN_AMT <- min(AMT17),
MAX_AMT <- max(AMT17),
n=n())
AMT <- AMT17+AMT16
AMT <- readxl_data$AMT17+readxl_data$AMT16
readxl_data %>%
group_by(Gender) %>%
summarise(Sum_AMT <- sum(AMT),
MEAN_AMT <- mean(AMT),
MIN_AMT <- min(AMT),
MAX_AMT <- max(AMT),
n=n())
install.packages("rpart")
library(rpart)
library(party)
install.packages("party")
install.packages("party")
install.packages("party")
library(party)
install.packages("party")
install.packages("rpart")
library(rpart)
install.packages("party")
install.packages("party")
library(party)
install.packages("caret")
library(caret)
install.packages("rpart.plot")
library(rpart.plot)
pkgs <- installed.packages()
colnames(pkgs)
1
2
colnames(pkgs[1])
colnames(pkgs)
library()
data("iris")
View(iris)
str(iris)
# 타겟 변수인 Species를 기준으로 각 종류별로 80%씩 추출하여 훈련용(train) 데이터 셋으로
iris_row_idx <- createDataPartition(iris$Species, p=0.8, list=FALSE) # list=FALSE => 추출한 정보를 factor로
iris_train <- iris[iris_row_idx,]
str(iris_train)
# iris-train 데이터 셋의 꽃 종류별 데이터 수 확인
table(iris_train$Species)
iris_test <- iris[-iris_row_idx,]
# iris_row_idx 벡터 내에 존재하는 인덱스는 제외한 행 추출
# "-" 기호 사용
table(iris_test$Species)
# summary
summary(iris_train)
summary(iris_test)
# 의사결정나무 생성하기
library(rpart)
#아래에서 오류발생생
rpart(formula, iris_test, control)
iris_result <- rpart(Species ~., data=iris_train, control=rpart.control(minsplit=2))
library(rpart.plot)
rpart.plot(iris_result)
#가지치기
iris_result$cptable
plotcp(iris_result)
iris_prune <- prune(iris_result, cp=0.0125)
rpart.plot(iris_prune)
install.packages("party")
library(party)
iris_ctree_result <- ctree(Species ~., data=iris_train, control = ctree_control(minsplit=2))
iris_ctree_result
plot(iris_train$Species)
?ctree_control
# install.packages("party")
library(party)
iris_ctree_result <- ctree(Species ~., data=iris_train, control = ctree_control(minsplit=2))
iris_ctree_result
plot(iris_train$Species) # 도식화
summary(iris_test)
# summary
summary(iris_train)
summary(iris_test)
data("iris")
View(iris)
str(iris)
# 타겟 변수인 Species를 기준으로 각 종류별로 80%씩 추출하여 훈련용(train) 데이터 셋으로
iris_row_idx <- createDataPartition(iris$Species, p=0.8, list=FALSE) # list=FALSE => 추출한 정보를 factor로
iris_train <- iris[iris_row_idx,]
str(iris_train)
# iris-train 데이터 셋의 꽃 종류별 데이터 수 확인
table(iris_train$Species)
iris_test <- iris[-iris_row_idx,]
# iris_row_idx 벡터 내에 존재하는 인덱스는 제외한 행 추출
# "-" 기호 사용
table(iris_test$Species)
# summary
summary(iris_train)
summary(iris_test)
# 의사결정나무 생성하기
library(rpart)
# 아래에서 오류발생생
rpart(formula, iris_test, control)
iris_result <- rpart(Species ~., data=iris_train, control=rpart.control(minsplit=2))
library(rpart.plot)
rpart.plot(iris_result)
# 가지치기
iris_result$cptable
plotcp(iris_result)
iris_prune <- prune(iris_result, cp=0.0125)
rpart.plot(iris_prune)
# install.packages("party")
library(party)
iris_ctree_result <- ctree(Species ~., data=iris_train, control = ctree_control(minsplit=2))
iris_ctree_result
plot(iris_train$Species) # 도식화
data("iris")
View(iris)
str(iris)
# 타겟 변수인 Species를 기준으로 각 종류별로 80%씩 추출하여 훈련용(train) 데이터 셋으로
iris_row_idx <- createDataPartition(iris$Species, p=0.8, list=FALSE) # list=FALSE => 추출한 정보를 factor로
# 타겟 변수인 Species를 기준으로 각 종류별로 80%씩 추출하여 훈련용(train) 데이터 셋으로
iris_row_idx <- createDataPartition(iris$Species, p=0.8, list=FALSE) # list=FALSE => 추출한 정보를 factor로
library(caret)
# 타겟 변수인 Species를 기준으로 각 종류별로 80%씩 추출하여 훈련용(train) 데이터 셋으로
iris_row_idx <- createDataPartition(iris$Species, p=0.8, list=FALSE) # list=FALSE => 추출한 정보를 factor로
iris_train <- iris[iris_row_idx,]
str(iris_train)
# iris-train 데이터 셋의 꽃 종류별 데이터 수 확인
table(iris_train$Species)
iris_test <- iris[-iris_row_idx,]
# iris_row_idx 벡터 내에 존재하는 인덱스는 제외한 행 추출
# "-" 기호 사용
table(iris_test$Species)
# summary
summary(iris_train)
summary(iris_test)
# 의사결정나무 생성하기
library(rpart)
# 아래에서 오류발생생
rpart(formula, iris_test, control)
iris_result <- rpart(Species ~., data=iris_train, control=rpart.control(minsplit=2))
library(rpart.plot)
rpart.plot(iris_result)
# 가지치기
iris_result$cptable
plotcp(iris_result)
iris_prune <- prune(iris_result, cp=0.0125)
rpart.plot(iris_prune)
# install.packages("party")
library(party)
iris_ctree_result <- ctree(Species ~., data=iris_train, control = ctree_control(minsplit=2))
iris_ctree_result
plot(iris_train$Species) # 도식화
Response : Species
Inputs : Sepal.Length, Sepal.Width, Petal.Length, Petal.Width
Number of observations: 120
1) Petal.Length <= 1.9; criterion = 1, statistic = 111.779
2)* weights = 40
1) Petal.Length > 1.9
3) Petal.Width <= 1.7; criterion = 1, statistic = 54.743
4) Petal.Length <= 4.7; criterion = 0.986, statistic = 8.583
5)* weights = 36
4) Petal.Length > 4.7
6)* weights = 7
3) Petal.Width > 1.7
7)* weights = 37
iris_ctree_result <- ctree(Species ~., data=iris_train, control = ctree_control(minsplit=2))
iris_ctree_result
plot(iris_train$Species) # 도식화
plot(iris_train$Species) # 도식화
plot(iris_train$Species) # 도식화
plot(iris_train$Species) # 도식화
plot(iris_ctree_result$Species) # 도식화
plot(iris_ctree_result) # 도식화
rpart(analysis_object, newdata, type)
# 모형 적용하여 예측 값 생성
expect <- predict(iris_prune, iris_test, type="class")
# 실제 종속변수
actual <- iris_test$Species
# 데이터셋 만들기
# 데이터셋 만들기
iris_performance <- data.frame(actual, expect)
# 3. 혼동행렬을 이용하여 모형평가 하기
confusionMatrix(expect,actual,mode="everything")
iris_performance1 <- data.frame(actual,expect1)
table(iris_performance1)
expect1 <- predict(iris_ctree_result, iris_test, type="response")
iris_performance1 <- data.frame(actual,expect1)
table(iris_performance1)
confusionMatrix(expect1,actual,mode="everything")
data("iris")
View(iris)
str(iris)
library(caret)
# 타겟 변수인 Species를 기준으로 각 종류별로 80%씩 추출하여 훈련용(train) 데이터 셋으로
iris_row_idx <- createDataPartition(iris$Species, p=0.8, list=FALSE) # list=FALSE => 추출한 정보를 factor로
iris_train <- iris[iris_row_idx,]
str(iris_train)
# iris-train 데이터 셋의 꽃 종류별 데이터 수 확인
table(iris_train$Species)
iris_test <- iris[-iris_row_idx,]
# iris_row_idx 벡터 내에 존재하는 인덱스는 제외한 행 추출
# "-" 기호 사용
table(iris_test$Species)
# summary
summary(iris_train)
summary(iris_test)
# 의사결정나무 생성하기
library(rpart)
# 아래에서 오류발생생
rpart(formula, iris_test, control)
iris_result <- rpart(Species ~., data=iris_train, control=rpart.control(minsplit=2))
library(rpart.plot)
rpart.plot(iris_result)
# 가지치기
iris_result$cptable
plotcp(iris_result)
iris_prune <- prune(iris_result, cp=0.0125)
rpart.plot(iris_prune)
# install.packages("party")
library(party)
iris_ctree_result <- ctree(Species ~., data=iris_train, control = ctree_control(minsplit=2))
iris_ctree_result
plot(iris_ctree_result) # 도식화
# 모형 적용하여 예측 값 생성
expect <- predict(iris_prune, iris_test, type="class")
# 실제 종속변수
actual <- iris_test$Species
# 데이터셋 만들기
iris_performance <- data.frame(actual, expect)
# 3. 혼동행렬을 이용하여 모형평가 하기
confusionMatrix(expect,actual,mode="everything")
expect1 <- predict(iris_ctree_result, iris_test, type="response")
iris_performance1 <- data.frame(actual,expect1)
table(iris_performance1)
confusionMatrix(expect1,actual,mode="everything")
bagging(formaula, data=train_data, mfial = number)
library(adabag)
install.packages("adabag")
library(adabag)
iris.bagging <- bagging(Species~., data=iris, mfinal=10)
#분류시 변수별 중요도, 모델 결과$importance
iris.bagging$importance
# 도식화
plot(iris.bagging$trees[[10]])
text(iris.bagging$trees[[10]])
expect1 <- predict(iris_ctree_result, iris_test, type="response")
iris_performance1 <- data.frame(actual,expect1)
table(iris_performance1)
confusionMatrix(expect1,actual,mode="everything")
expect2 <- predict(iris.bagging, iris_test, type="response")
# -> Species를 예측하여 분류한 변수를 팩터화 한다.
expect2$class<-as.factor(expect2$class)
confusionMatrix(expect2$class,iris_test$Species,mode="everythi
ng")
confusionMatrix(expect2$class,iris_test$Species,mode="everythi
ng")
expect2 <- predict(iris.bagging, iris_test, type="response")
# -> Species를 예측하여 분류한 변수를 팩터화 한다.
expect2$class<-as.factor(expect2$class)
expect2 <- predict(iris.bagging, iris_test, type="response")
# -> Species를 예측하여 분류한 변수를 팩터화 한다.
expect2$class<-as.factor(expect2$class)
confusionMatrix(expect2$class,iris_test$Species,mode="everythi
ng")
confusionMatrix(expect2$class,iris_test$Species,mode="everything")
iris.ada <- boosting(Species~., data=iris_train, mfinal=10)
iris.ada$importance # 분류 영향정도
barchart(iris.ada$importance) # 도식화하여 변수의 중요도
plot(iris.ada$trees[[10]]) # tree 그림 그리기
text(iris.ada$trees[[10]])
expect3 <- predict(iris.ada, iris_test, type="response")
expect3$class<-as.factor(expect3$class)
confusionMatrix(expect3$class,iris_test$Species,mode="everything")
colSums(is.na(stagec))
stagec1 <- na.omit(stagec)
set.seed(1234) => 초기값 설정
set.seed(1234) # 초기값 설정
# Random Sampling을 통해 70%와 30%로 구분한다
ind <- sample(2, nrow(stagec3), replace=TRUE, prob=c(0.7, 0.3))
set.seed(1234) # 초기값 설정
# Random Sampling을 통해 70%와 30%로 구분한다
ind <- sample(2, nrow(stagec3), replace=TRUE, prob=c(0.7, 0.3))
# Random Sampling을 통해 70%와 30%로 구분한다
ind <- sample(2, nrow(stagec3), replace=TRUE, prob=c(0.7, 0.3))
# Random Sampling을 통해 70%와 30%로 구분한다
ind <- sample(2, nrow(stagec3), replace=TRUE, prob=c(0.7, 0.3))
library(rpart)
data(stagec)
colSums(is.na(stagec))
stagec1 <- na.omit(stagec)
set.seed(1234) # 초기값 설정
# Random Sampling을 통해 70%와 30%로 구분한다
ind <- sample(2, nrow(stagec3), replace=TRUE, prob=c(0.7, 0.3))
colSums(is.na(stagec))
stagec1 <- na.omit(stagec)
set.seed(1234) # 초기값 설정
# Random Sampling을 통해 70%와 30%로 구분한다
ind <- sample(2, nrow(stagec3), replace=TRUE, prob=c(0.7, 0.3))
trainData <- stagec1[ind==1, ] # n=102개
testData <- stagec1[ind==2, ] # n=32
library(rpart)
data(stagec)
View(stagec)
# Random Sampling을 통해 70%와 30%로 구분한다
ind <- sample(2, nrow(stagec3), replace=TRUE, prob=c(0.7, 0.3))
# Random Sampling을 통해 70%와 30%로 구분한다
ind <- sample(2, nrow(stagec1), replace=TRUE, prob=c(0.7, 0.3))
trainData <- stagec1[ind==1, ] # n=102개
testData <- stagec1[ind==2, ] # n=32
# randomForest 설치 및 로딩
install.packages("randomForest")
library(randomForest)
# 알고리즘 적용
randomForest(Formula ., data=trainData, ntree= number)
# 알고리즘 적용
randomForest(Formula ., data=trainData, ntree= number)
library(randomForest)
# 알고리즘 적용
randomForest(Formula ., data=trainData, ntree= number)
library(randomForest)
# 알고리즘 적용
randomForest(Formula ., data=trainData, ntree= number)
# 알고리즘 적용
randomForest(Formula ., data=trainData, ntree= number)
# 알고리즘 적용
randomForest(Formula ., data=trainData, ntree= number)
# 알고리즘 적용
randomForest(Formula ., data=trainData, ntree= number)
# 알고리즘 적용
randomForest(Formula ., data=trainData, ntree= number)
# 알고리즘 적용
randomForest(Formula , data=trainData, ntree= number)
.
# 알고리즘 적용
randomForest(Formula ., data=trainData, ntree= number)
# 알고리즘 적용
randomForest(Formula . data=trainData, ntree= number)
,
# 알고리즘 적용
randomForest(Formula ., data=trainData, ntree= number)
# 알고리즘 적용
randomForest(Formula, data=trainData, ntree= number)
rf <- randomForest(ploidy ~ ., data=trainData, ntree=100)
importance(rf)
varImpPlot(rf)
plot(rf)
legend("topright",colnames(rf$err.rate),cex=0.8,fill=1:4) #범례 정리
library(caret)
predict <- predict(rf, testData)
confusionMatrix(predict, testData$ploidy ,method=“everything”
confusionMatrix(predict, testData$ploidy, method=“everything”
confusionMatrix(predict, testData$ploidy, method="everything"
predict <- predict(rf, testData)
confusionMatrix(predict, testData$ploidy, method="everything"
predict <- predict(rf, testData)
library(caret)
predict <- predict(rf, testData)
confusionMatrix(predict, testData$ploidy, method="everything"
legend("topright",colnames(rf$err.rate),cex=0.8,fill=1:4) #범례 정리
importance(rf)
varImpPlot(rf)
plot(rf)
legend("topright",colnames(rf$err.rate),cex=0.8,fill=1:4) #범례 정리
library(caret)
predict <- predict(rf, testData)
confusionMatrix(predict, testData$ploidy, method="everything"
# 1.필요 라이브러리 로딩
#데이터 핸들링
library(readxl)
library(dplyr)
# 기술통계량
library(psych)
library(descr)
# 분류기법
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)
library(caret)
# 시각화 (그래프로 표현)
library(ggplot2)
box_office <- read_excel("train_box_office.xlsx")
box_office <- read_excel("train_box_office.xlsx")
box_office <- read_excel("./train_box_office.xlsx")
box_office <- read_excel("train_box_office.xlsx")
# 1.필요 라이브러리 로딩
#데이터 핸들링
library(readxl)
library(dplyr)
# 기술통계량
library(psych)
library(descr)
# 분류기법
library(rpart)
library(rpart.plot)
library(adabag)
library(randomForest)
library(caret)
# 시각화 (그래프로 표현)
library(ggplot2)
box_office <- read_excel("train_box_office.xlsx")
head(box_office)
head(box_office)
tail(box_office)
box_office <- read_excel("train_box_office.xlsx")
head(box_office)
tail(box_office)
View(box_office)
colSums(is.na(box_office))
predict <- predict(rf, testData)
confusionMatrix(predict, testData$ploidy, method="everything"
